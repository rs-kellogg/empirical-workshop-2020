{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text analysis with spaCy and textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is spaCy?\n",
    "https://spacy.io\n",
    "\n",
    "spaCy is a library for advanced Natural Language Processing, written in Python and Cython. spaCy utilizes convolution network models for English, German, Spanish, Portuguese, French, Italian, Dutch and multi-language NER, as well as tokenization for various other languages.\n",
    "\n",
    "spaCy is designed for large scale text extraction, using Cython to provide increased processing speed. spaCy also supports deep learning workflows that allow connecting statistical models trained by popular machine learning libraries like TensorFlow, Keras, Scikit-learn or PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy # visualization tools for spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: 'en_core_web_lg'\n",
    "\n",
    "English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, syntactic dependency parse and named entities.\n",
    "\n",
    "685k keys, 685k unique vectors (300 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Extracting currency amounts, the nouns they refer to, and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntactic dependency relationships in practice\n",
    "# currency values and the nouns they refer to\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "TEXTS = [\n",
    "    \"\"\"Google just made another giant move in its Silicon Valley land grab.\n",
    "\n",
    "The internet company spent $1 billion on a large office park near its headquarters in Mountain View, California, according to the Mercury News, and has now spent at least $2.8 billion on properties in Mountain View, Sunnyvale and San Jose over the last two years.\n",
    "In this case, Google is purchasing property that it's already been leasing. The company is the main tenant of the 12 buildings that comprise the 51.8-acre Shoreline Technology Park.\n",
    "\n",
    "Google declined to comment on its purchase.\n",
    "\n",
    "Earlier this month, Google agreed to pay an additional $110 million for 10.5 acres for a new campus in downtown San Jose, with the possibility of buying about 11 more acres. The city will vote on the plans in early December.\n",
    "It's also been a big year for Google property purchases outside of Silicon Valley.\n",
    "\n",
    "In the first quarter, the company spent $2.4 billion to buy New York City's Chelsea Market. Chief Financial Officer Ruth Porat said that the company favors \"owning rather than leasing real estate when we see good opportunities.\"\n",
    "\n",
    "As for leases, Google just signed on for a massive new space in downtown San Francisco.\"\"\"\n",
    "]\n",
    "\n",
    "class stop_loop(Exception): pass\n",
    "\n",
    "def qualifier_value(money_txt):\n",
    "    money_doc = nlp(str(money_txt))\n",
    "    pos_list = [token.pos_ for token in money_doc]\n",
    "    money_list = [token.text for token in money_doc]\n",
    "    money_start = min(loc for loc, pos in enumerate(pos_list) if (pos == 'SYM' or pos == 'NUM'))\n",
    "    qualifier = ' '.join(money_list[:money_start])\n",
    "    value = ' '.join(money_list[money_start:])\n",
    "    \n",
    "    return qualifier, value\n",
    "    \n",
    "    \n",
    "def get_root_verb(token):\n",
    "    verb = None\n",
    "    while not verb:\n",
    "        if token.pos_ == 'VERB':\n",
    "            verb = token\n",
    "        else:\n",
    "            token = token.head\n",
    "        if not token.head:\n",
    "            break\n",
    "            \n",
    "    return [verb.text, verb.idx]\n",
    "\n",
    "\n",
    "def get_locations(money_ent):\n",
    "    gpe_list = [gpe for gpe in filter(lambda w: w.ent_type_ == 'GPE', money_ent.sent)]\n",
    "    location_list = []\n",
    "    for gpe in gpe_list:\n",
    "        money_verb_index = get_root_verb(money_ent)\n",
    "        gpe_verb_index = get_root_verb(gpe)\n",
    "        if money_verb_index == gpe_verb_index:\n",
    "            location_list.append(gpe.text)\n",
    "    if not location_list:\n",
    "        gpe_doc = nlp(str(money_ent.sent))\n",
    "        location_list = [gpe.text for gpe in filter(lambda w: w.ent_type_ == 'GPE', gpe_doc)]\n",
    "\n",
    "    return location_list\n",
    "\n",
    "\n",
    "def extract_currency_relations(doc):\n",
    "    # merge entities and noun chunks into one token\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "\n",
    "    relations = []\n",
    "    for money in filter(lambda w: w.ent_type_ == 'MONEY', doc):\n",
    "        try:\n",
    "            # syntactic relationship 1\n",
    "            advcl = [w for w in money.head.children if w.dep_ == 'advcl']\n",
    "            if advcl:\n",
    "                for child in advcl[0].children:\n",
    "                    if child.dep_ == 'dobj':\n",
    "                        parse_type = 1\n",
    "                        qual, val = qualifier_value(money.text)\n",
    "                        locations = get_locations(money)\n",
    "                        relations.append((qual, val, child, locations, parse_type))\n",
    "                        raise stop_loop()\n",
    "                        \n",
    "            # syntactic relationship 2\n",
    "            cprep = [w for w in money.children if w.dep_ == 'prep']\n",
    "            if cprep:\n",
    "                for child in cprep[0].children:\n",
    "                    if child.dep_ == 'pobj':\n",
    "                        parse_type = 2\n",
    "                        qual, val = qualifier_value(money.text)\n",
    "                        locations = get_locations(money)\n",
    "                        relations.append((qual, val, child, locations, parse_type))\n",
    "                        raise stop_loop()\n",
    "            \n",
    "            # syntactic relationship 3\n",
    "            hprep = [w for w in money.head.children if w.dep_ == 'prep']\n",
    "            if hprep:\n",
    "                for child in hprep[0].children:\n",
    "                    if child.dep_ == 'pobj':\n",
    "                        parse_type = 3\n",
    "                        qual, val = qualifier_value(money.text)\n",
    "                        locations = get_locations(money)\n",
    "                        relations.append((qual, val, child, locations, parse_type))\n",
    "                        raise stop_loop()\n",
    "                        \n",
    "            # syntactic relationship 4\n",
    "            if money.dep_ in ('attr', 'dobj'):\n",
    "                subject = [w for w in money.head.lefts if w.dep_ == 'nsubj']\n",
    "                if subject:\n",
    "                    parse_type = 4\n",
    "                    subject = subject[0]\n",
    "                    qual, val = qualifier_value(money.text)\n",
    "                    locations = get_locations(money)\n",
    "                    relations.append((qual, val, subject, locations, parse_type))\n",
    "                    raise stop_loop()\n",
    "                    \n",
    "            # syntactic relationship 5\n",
    "            elif money.dep_ == 'pobj' and money.head.dep_ == 'prep':\n",
    "                parse_type = 5\n",
    "                qual, val = qualifier_value(money.text)\n",
    "                locations = get_locations(money)\n",
    "                relations.append((qual, val, money.head.head, locations, parse_type))\n",
    "                raise stop_loop()\n",
    "                \n",
    "        except stop_loop:\n",
    "            pass\n",
    "                 \n",
    "    return relations\n",
    "\n",
    "df = pd.DataFrame(columns='QUALIFIER VALUE ASSET LOCATION'.split())\n",
    "\n",
    "for text in TEXTS:\n",
    "    print(text)\n",
    "    doc = nlp(str(text))\n",
    "    relations = extract_currency_relations(doc)\n",
    "    for r0, r1, r2, r3, r4 in relations:\n",
    "        relation_dict = {'QUALIFIER':r0, 'VALUE':r1, 'ASSET':r2.text, 'LOCATION':r3}\n",
    "        df = df.append(relation_dict, ignore_index=True)\n",
    "\n",
    "\n",
    "display(HTML(df.to_html(index=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert monetary values to integer using regex substitution\n",
    "\n",
    "import re\n",
    "\n",
    "int_values = []\n",
    "for text_value in df['VALUE']:\n",
    "    if 'million' in text_value:\n",
    "        money_expr = re.sub('million', '*1000000', text_value.strip())\n",
    "    elif 'billion' in text_value:\n",
    "        money_expr = re.sub('billion', '*1000000000', text_value.strip())\n",
    "    money_expr = re.sub(r'\\$', '', money_expr)\n",
    "    money_int = eval(money_expr)\n",
    "    int_values.append(int(money_int))\n",
    "    \n",
    "df = df.assign(VALUE=int_values)\n",
    "\n",
    "display(HTML(df.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "spaCy automatically tokenizes text and provides several context relevant properties for each token.\n",
    "\n",
    "Let's look at the following sentence:\n",
    "\n",
    "**In downtown Evanston, Rhonda Smith bought 1 iPhone at 8 a.m. on October 5th because they were 30% off at BestBuy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process document with spaCy nlp model\n",
    "doc = nlp(u'In downtown Evanston, Rhonda Smith bought 1 iPhone at 8 a.m. on October 5th because they were 30% off at BestBuy.')\n",
    "\n",
    "# get tokenized representation of sentence\n",
    "tokenized = [token for token in doc]\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# named entities can be use for disambiguation\n",
    "\n",
    "doc = nlp(u\"Tim Cook, CEO of Apple, has many apple trees on his property.\")\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print properties of each token in sentence\n",
    "\n",
    "df = pd.DataFrame(columns='TEXT LEMMA POS TAG DEP SHAPE ALPHA ENT'.split())\n",
    "\n",
    "for token in doc:\n",
    "    tokendict = {'TEXT':token.text,\n",
    "                 'LEMMA':token.lemma_,\n",
    "                 'POS':token.pos_,\n",
    "                 'TAG':token.tag_,\n",
    "                 'DEP':token.dep_,\n",
    "                 'SHAPE':token.shape_,\n",
    "                 'ALPHA':token.is_alpha,\n",
    "                 'ENT':token.ent_type_}\n",
    "    df = df.append(tokendict, ignore_index=True)\n",
    "\n",
    "display(HTML(df.to_html(index=False)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic dependency relationships\n",
    "\n",
    "Syntactic dependencies are the grammatical relationships between words. spaCy can be used to extract this dependency information from sentences in a text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of syntactic dependency \n",
    "doc = nlp(str(\"In the first quarter, the company spent $2.4 billion to buy New York City's Chelsea Market.\"))\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## textacy \n",
    "https://chartbeat-labs.github.io/textacy/index.html\n",
    "\n",
    "textacy builds upon spaCy's framework and provides convenient functions for many advanced NLP tools. textacy also performs basic text feature counts and computes several readability measures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load earnings call transcript\n",
    "# Flex Ltd. Q3 2020\n",
    "\n",
    "with open('FLEX_earnings_call.txt', 'r') as f:\n",
    "    transcript = f.read()\n",
    "    doc = nlp(transcript)\n",
    "\n",
    "# print first few lines from CEO\n",
    "for line in transcript.splitlines()[23:33]:print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import textacy package\n",
    "\n",
    "import textacy\n",
    "\n",
    "# compute counts and readability stats\n",
    "ts = textacy.TextStats(doc)\n",
    "\n",
    "print('Unique words')\n",
    "print(ts.n_unique_words)\n",
    "print('-----------------')\n",
    "print('Basic counts')\n",
    "print(ts.basic_counts)\n",
    "print('-----------------')\n",
    "print('Readabiltiy stats')\n",
    "print(ts.readability_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examination of Presidental Inaugural addresses\n",
    "# download speeches from nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('inaugural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# plot inaugural speech grade-level over time\n",
    "import matplotlib.pyplot as pyplot\n",
    "from nltk.corpus import inaugural\n",
    "\n",
    "names = inaugural.fileids()\n",
    "print(names)\n",
    "\n",
    "years = []\n",
    "grade_lvls = []\n",
    "for name in names:\n",
    "    filetext = inaugural.raw(fileids=name)\n",
    "    year = int(name.split('-')[0])\n",
    "    years.append(year)\n",
    "    \n",
    "    doc = nlp(filetext)\n",
    "    ts = textacy.TextStats(doc)\n",
    "    grade_lvl = ts.readability_stats['flesch_kincaid_grade_level']\n",
    "    grade_lvls.append(grade_lvl)\n",
    "\n",
    "pyplot.plot(years, grade_lvls, 'bo')\n",
    "pyplot.xlabel('year')\n",
    "pyplot.ylabel('grade level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling with spaCy and textacy\n",
    "\n",
    "Topic models can provide a means to analyze and categorize a corpus of texts. Topics often refer to clusters of words that frequently occur together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spaCy to generate list of terms from corpus of documents\n",
    "import os\n",
    "import textacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# import the inaugural addresses\n",
    "from nltk.corpus import inaugural\n",
    "\n",
    "names = inaugural.fileids()\n",
    "\n",
    "# create list of terms from token lemmas in texts\n",
    "terms_list = []\n",
    "\n",
    "for name in names:\n",
    "    filetext = inaugural.raw(fileids=name)\n",
    "    doc = nlp(filetext)\n",
    "    terms_list.append([token.lemma_ for token in doc if token.text.lower() not in STOP_WORDS and token.text.isalnum()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    " \n",
    "# create word vectors from speech terms\n",
    "from textacy.tm import TopicModel\n",
    "from textacy.vsm import Vectorizer\n",
    "\n",
    "vectorizer = Vectorizer(tf_type='linear', apply_idf=True, idf_type='smooth')\n",
    "doc_term_matrix = vectorizer.fit_transform(terms_list)\n",
    "\n",
    "#initialize and train a topic model:\n",
    "model = TopicModel('nmf', n_topics=5)\n",
    "model.fit(doc_term_matrix)\n",
    "\n",
    "print (\"======================model=================\")\n",
    "print (model)\n",
    " \n",
    "doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, topics=[0,1]):\n",
    "    print('topic', topic_idx, ':', '   '.join(top_terms))\n",
    "    \n",
    "for i, val in enumerate(model.topic_weights(doc_topic_matrix)):\n",
    "     print(i, val)\n",
    "           \n",
    "model.termite_plot(doc_term_matrix, vectorizer.id_to_term, topics=-1,  n_terms=25, sort_terms_by='seriation')  \n",
    "model.save('nmf-25topics_inaugural.pkl')        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And much much more...\n",
    "https://spacy.io/usage/linguistic-features\n",
    "https://spacy.io/usage/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
